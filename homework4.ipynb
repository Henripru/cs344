{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "homework4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcO_ax5-DGS6",
        "colab_type": "text"
      },
      "source": [
        "# 1 \n",
        "  I believe that deep neural networks really are a breakthrough which will be used for years in the future. Just look at how they have been used already. In 1996 Deep Blue beat Garry Kasparov who to this day is one of the world's greatest chess players. The modern iteration of which is AlphaZero which has mastered both Chess and Go at a whole different level. Besides board games, neural networks can be trained to read images and classify different people. Apple phones create albums in which a certain persons face appears. People use these networks for convenience and research. \n",
        "\n",
        "  In a more technical light. We can see why these deep neural networks are here to stay more than the old perceptrons or expert systems in that the new deep neural networks can be built with self learning. Modern systems such as LSTM or algorithms like the Monte Carlo enable the model to improve upon each training iteration. Unlike the old models which couldn't do this to the same extent. \n",
        "\n",
        "  # 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tot6VVoaDD5y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "baa5a8cf-491c-418c-a91e-76d9403547eb"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "print(\"Initial Weights:\")\n",
        "xorInput = np.array([1, 1])\n",
        "layerOneWeights = np.array([[0.11, 0.12], [0.21, 0.08]])\n",
        "layerTwoWeights = np.array([[0.14], [0.15]])\n",
        "\n",
        "print(\"xor input \", xorInput)\n",
        "print(\"layer 1 weights \\n\", layerOneWeights)\n",
        "print(\"layer 2 weights \\n\", layerTwoWeights, \"\\n\")\n",
        "\n",
        "print(\"Output:\")\n",
        "product1 = xorInput.dot(layerOneWeights)\n",
        "oj = product1.dot(layerTwoWeights)\n",
        "print(\"forward pass result = \", oj, \"\\n\")\n",
        "\n",
        "print(\"Error:\")\n",
        "l2error = (0 - oj[0])**2\n",
        "print(\"L2Error = \", l2error)\n",
        "delta = (0 - oj[0])\n",
        "print(\"Delta = \", delta, \"\\n\")\n",
        "\n",
        "print(\"Back Propogate Updates: \")\n",
        "learningRate = 0.05\n",
        "print(\"learning rate = \", learningRate)\n",
        "newLayerTwoWeights = np.add(layerTwoWeights, learningRate * product1.reshape(2,1) * 1.0 * delta)\n",
        "print(\"updated layer two weights \\n\", newLayerTwoWeights)\n",
        "newLayerOneWeights = np.add(layerOneWeights, learningRate * delta * np.array([[1], [1]]).dot(layerTwoWeights.reshape(1,2)))\n",
        "print(\"updated layer one weights \\n\", newLayerOneWeights)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initial Weights:\n",
            "xor input  [1 1]\n",
            "layer 1 weights \n",
            " [[0.11 0.12]\n",
            " [0.21 0.08]]\n",
            "layer 2 weights \n",
            " [[0.14]\n",
            " [0.15]] \n",
            "\n",
            "Output:\n",
            "forward pass result =  [0.0748] \n",
            "\n",
            "Error:\n",
            "L2Error =  0.005595040000000001\n",
            "Delta =  -0.0748 \n",
            "\n",
            "Back Propogate Updates: \n",
            "learning rate =  0.05\n",
            "updated layer two weights \n",
            " [[0.1388032]\n",
            " [0.149252 ]]\n",
            "updated layer one weights \n",
            " [[0.1094764 0.119439 ]\n",
            " [0.2094764 0.079439 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bv1X1Im1TNJ8",
        "colab_type": "text"
      },
      "source": [
        "# 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6xrAaHkTlsB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        },
        "outputId": "12f33824-6d6a-4c8a-d8b2-9432f514c221"
      },
      "source": [
        "from keras import layers\n",
        "from keras import models\n",
        "\n",
        "model = models.Sequential()\n",
        "\n",
        "# Configure a convnet with 3 layers of convolutions and max pooling.\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
        "\n",
        "# Add layers to flatten the 2D image and then do a 10-way classification.\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "from keras.datasets import fashion_mnist\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "train_images = train_images.reshape((60000, 28, 28, 1))\n",
        "train_images = train_images.astype('float32') / 255\n",
        "\n",
        "test_images = test_images.reshape((10000, 28, 28, 1))\n",
        "test_images = test_images.astype('float32') / 255\n",
        "\n",
        "train_labels = to_categorical(train_labels)\n",
        "test_labels = to_categorical(test_labels)\n",
        "\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.fit(train_images, train_labels, epochs=5, batch_size=64)\n",
        "model.evaluate(test_images, test_labels)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_26 (Conv2D)           (None, 26, 26, 64)        640       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_18 (MaxPooling (None, 13, 13, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_27 (Conv2D)           (None, 11, 11, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_19 (MaxPooling (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_28 (Conv2D)           (None, 3, 3, 32)          18464     \n",
            "_________________________________________________________________\n",
            "flatten_8 (Flatten)          (None, 288)               0         \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 64)                18496     \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 10)                650       \n",
            "=================================================================\n",
            "Total params: 75,178\n",
            "Trainable params: 75,178\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 37s 613us/step - loss: 0.5485 - accuracy: 0.7961\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 37s 616us/step - loss: 0.3401 - accuracy: 0.8765\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 37s 615us/step - loss: 0.2888 - accuracy: 0.8934\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 37s 613us/step - loss: 0.2577 - accuracy: 0.9053\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 37s 611us/step - loss: 0.2357 - accuracy: 0.9132\n",
            "10000/10000 [==============================] - 2s 194us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.314449747633934, 0.8895999789237976]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnfsvYYvexFu",
        "colab_type": "text"
      },
      "source": [
        "In summary, I used the cnn example from the cs344-code repo. I used the fashion_mnist dataset instead of the mnist dataset, and then messed around with tweaking the number of layers as well as the nodes within those layers. I came to the conclusion that 3 layers worked best with the node counts seen in the code above. The accuracy was highest at 91.132. This number is not the best as a performance measurement, but it is the highest I could achieve with this cnn layout."
      ]
    }
  ]
}